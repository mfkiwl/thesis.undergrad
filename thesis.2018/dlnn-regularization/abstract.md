Regularization in Deep Learning
===============================

Deep neural networks contain multiple non-linear hidden layers 
and this makes them very expressive models that can learn very 
complicated relationships between their inputs and outputs. With
limited data, however, many of these complicated relationships 
will be the result of sampling noise. This leads to overfitting 
and many methods have been developed for reducing it such as $L^1$,
$L^2$ regularization and the dropout method.


